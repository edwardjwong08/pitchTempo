# -*- coding: utf-8 -*-
"""createDB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t6irMDWE_lJRm0icU2dt-acFxoU5y8LN
"""

import numpy as np
import matplotlib.pyplot as plt
#from dask import dataframe as dd

from google.colab import drive
drive.mount('/content/drive')

import sqlite3
from sqlalchemy import create_engine
import datetime as dt
#creates database
#do not run multiple times
disk_engine = create_engine("sqlite:////content/drive/Shared drives/SABER_Talk/pbp.db")

import pandas as pd
#WARNING: this takes approx an hour to complete and can be extreme on RAM if not careful
#do timing for pull
start = dt.datetime.now()
#break csv to chunks
chunksize = 20000
j = 0
index_start = 1

#loop runs the csv for each cunk
for df in pd.read_csv('/content/drive/Shared drives/SABER_Talk/oldData/pbp1321adjusted.csv', chunksize=chunksize, iterator=True, encoding='utf-8'):

    j+=1
    #gives time
    print('{} seconds: completed {} rows'.format((dt.datetime.now() - start).seconds, j*chunksize))
    #turns csv chunks to sql in table called data
    df.to_sql('data', disk_engine, if_exists='append')
    #adjusts the index of the table
    index_start = df.index[-1] + 1

import sqlite3
#creates connection
connection = sqlite3.connect("/content/drive/Shared drives/SABER_Talk/pbp.db")
#calls basic queries
def query(query):
  cursor = connection.execute(query)
  for row in cursor:
      print(row)

query("SELECT count(*) from data")
query("SELECT * FROM data LIMIT 10")
